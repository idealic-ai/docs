/**
 * @fileoverview Automated comprehension tests for 04. refining.examples.
 *
 * These tests are automatically generated from the source documentation files
 * (e.g., Markdown chapters) and are designed to evaluate the clarity and
 * effectiveness of the documentation in conveying core concepts.
 *
 * The testing methodology, including content isolation (Bible vs. Manifest)
 * and scoping (Chapter vs. Complete), is detailed in:
 * /docs/prompts/test-methodology.md
 *
 * !! DO NOT EDIT THIS FILE DIRECTLY !!
 * Changes should be made in the source documentation from which these tests
 * are generated. This will ensure that tests remain synchronized with the
 * content they are intended to evaluate.
 */
import { describe, it, expect } from 'vitest';
import { readFileSync } from 'fs';
import { join } from 'path';
import { Request, Provider } from '@augceo/agent';
import '@augceo/agent/provider/vertexai';
import { questions } from './04. refining.examples.questions';
import dotenv from 'dotenv';

dotenv.config();

const chapterBibleContent = readFileSync(join(__dirname, '04. refining.examples.bible.md'), 'utf-8');
const chapterManifestContent = readFileSync(join(__dirname, '04. refining.examples.manifest.md'), 'utf-8');
const completeBibleContent = readFileSync(join(__dirname, '../Bible.md'), 'utf-8');
const completeManifestContent = readFileSync(join(__dirname, '../Manifest.md'), 'utf-8');

// Helper function to test a question with given content
async function testQuestion(questionKey: keyof typeof questions, content: string, testName: string) {
  const config = {
    provider: 'vertexai',
    model: 'gemini-2.0-flash',
    project: process.env.VERTEXAI_PROJECT,
    location: process.env.VERTEXAI_LOCATION,
    temperature: 0.5,
    maxTokens: 3000,
  } as const satisfies Provider.MinimalConfig;

  const schema = questions[questionKey].schema;
  const questionDescription = schema.description || '';

  const userMessageContent = `

Provided Content to analyze:
${content}

Question based *only* on the provided content:
${questionDescription}

Important: This is a multiple choice question. Only select the answers that are clearly supported by the content above. Do not try to select all options or be overly inclusive - be precise and selective.`;

  const messages = [
    { role: 'system', content: 'You are an expert on the content provided. Answer the question based on the given content. For multiple choice questions, be selective and only choose the answers that are truly correct based on the content. Do not try to list all possible options - only select the ones that are actually supported by the provided material.' },
    { role: 'user', content: userMessageContent }
  ] as const;

  const response = await Request(config, schema, messages);
  expect(response).toBeDefined();
  expect(response[0]).toBeDefined();

  // Validate the response structure matches the schema
  expect(response[0]).toHaveProperty('answer');
  expect(response[0]).toHaveProperty('reasoning');
  expect(response[0]).toHaveProperty('interpretation');
  expect(response[0]).toHaveProperty('breakdown');
  expect(response[0]).toHaveProperty('confusion');
  expect(response[0]).toHaveProperty('suggestions');
  expect(Array.isArray(response[0].answer)).toBe(true);
  expect(typeof response[0].reasoning).toBe('string');

  // Check that the answer contains correct answers from the schema
  const correctAnswers = questions[questionKey].correctAnswers;
  const givenAnswers = response[0].answer;
  const aiReasoning = response[0].reasoning;
  const aiInterpretation = response[0].interpretation;
  const aiBreakdown = response[0].breakdown;
  const aiConfusion = response[0].confusion;
  const aiSuggestions = response[0].suggestions;
  const totalOptions = schema.properties.answer.items.enum.length;

  // console.log(`${testName} - Given answers:`, givenAnswers);
  // console.log(`${testName} - Correct answers:`, correctAnswers);
  // console.log(`${testName} - Total options available:`, totalOptions);
  // console.log(`${testName} - AI reasoning:`, aiReasoning);
  // console.log(`${testName} - AI interpretation:`, aiInterpretation);
  // console.log(`${testName} - AI breakdown:`, aiBreakdown);
  // console.log(`${testName} - AI confusion:`, aiConfusion);
  // console.log(`${testName} - AI suggestions:`, aiSuggestions);

  // Validate that AI didn't give too many answers (shouldn't select more than 70% of all options)
  const maxReasonableAnswers = Math.ceil(totalOptions * 0.7);
  expect(givenAnswers.length).toBeLessThanOrEqual(maxReasonableAnswers);

  // Validate that the AI didn't just select everything (should be selective)
  expect(givenAnswers.length).toBeLessThan(totalOptions);

  // Custom assertion for answer quality with detailed feedback
  const givenSet = new Set(givenAnswers);
  const correctSet = new Set(correctAnswers);
  const allOptionsSet = new Set(schema.properties.answer.items.enum);

  const correctlyChosen = givenAnswers.filter(answer => correctSet.has(answer)).sort();
  const missedCorrect = correctAnswers.filter(answer => !givenSet.has(answer)).sort();
  const incorrectlyChosen = givenAnswers.filter(answer => !correctSet.has(answer)).sort();

  // Create detailed status message
  const statusParts = [];
  if (correctlyChosen.length > 0) {
    statusParts.push(`  • ✓ Correctly understood: ${correctlyChosen.length}/${correctAnswers.length}`);
    correctlyChosen.forEach(answer => statusParts.push(`    - ${answer}`));
  }
  if (missedCorrect.length > 0) {
    statusParts.push(`  • ✗ Missed correct answers: ${missedCorrect.length}`);
    missedCorrect.forEach(answer => statusParts.push(`    - ${answer}`));
  }
  if (incorrectlyChosen.length > 0) {
    statusParts.push(`  • ✗ Incorrectly inferred: ${incorrectlyChosen.length}`);
    incorrectlyChosen.forEach(answer => statusParts.push(`    - ${answer}`));
  }

  const statusMessage = statusParts.join('\n');
  console.log(`${testName} - ${questionDescription}:\n${statusMessage}`);

  // Assert that AI got at least some correct answers and didn't miss too many
  const hasAllCorrectAnswers = missedCorrect.length === 0;
  const hasNoIncorrectAnswers = incorrectlyChosen.length === 0;
  const hasAtLeastSomeCorrect = correctlyChosen.length > 0;

  // Custom assertion with detailed error message including AI reasoning
  if (!hasAllCorrectAnswers || !hasNoIncorrectAnswers) {
    const errorMessage = `Answer quality assessment for ${testName}:\n${statusMessage}\n
AI's reasoning: "${aiReasoning}"\n
AI's interpretation: "${aiInterpretation}"\n
AI's breakdown: "${aiBreakdown}"\n
AI's confusion: "${aiConfusion}"\n
AI's suggestions: "${aiSuggestions}"\n
Expected: All correct answers chosen, no incorrect answers
Actual: ${correctlyChosen.length}/${correctAnswers.length} correct, ${incorrectlyChosen.length} incorrect`;
    expect.fail(errorMessage);
  }

  // Fallback: at minimum, ensure some correct answers were chosen
  if (!hasAtLeastSomeCorrect) {
    const errorMessage = `No correct answers were chosen at all for ${testName}\n${statusMessage}\n
AI's reasoning: "${aiReasoning}"\n
AI's interpretation: "${aiInterpretation}"\n
AI's breakdown: "${aiBreakdown}"\n
AI's confusion: "${aiConfusion}"\n
AI's suggestions: "${aiSuggestions}"`;

    expect.fail(errorMessage);
  }
}

// Top-level describe for the chapter's tests
describe.concurrent(`04. refining.examples Comprehension Tests`, () => {

  describe.concurrent('Bible - Chapter Scope', () => {

  });

  describe.concurrent('Manifest - Chapter Scope', () => {

  });

  describe.skip.concurrent('Bible - Complete Scope', () => {

  });

  describe.skip.concurrent('Manifest - Complete Scope', () => {

  });
}, 60000);
