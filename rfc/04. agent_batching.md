# The Batching Protocol

_For definitions of key terms used in this document, please refer to the [Glossary](./00. glossary.md)._

This document outlines a protocol for processing multiple, independent agent tasks within a single request, using a state-driven architecture.

## 1. Foundational Requirement: The State System

The core prerequisite for this batching protocol is the **State System**, which explicitly decouples the planning of actions from their execution.

The **State Object** is the bridge between these phases. It is a mutable, JSON-like object that serves two critical functions:

1.  **Target for Execution**: It is the canvas upon which tools operate. Every `Tool Call` includes an `_outputPath` property, which specifies a path within the `State` object where the tool's output should be written during execution.
2.  **Source for Dependencies**: A `Tool Call` can reference a value from the `State` as one of its inputs. This allows for the creation of dependency graphs.

## 2. The Batching Mechanism

The true power of this architecture is revealed in its native support for batch operations, which is enabled by the State System.

### 2.1. State Identifiers

To process multiple tasks in a single request, the system accepts an array of context messages. Each message representing a distinct task is assigned a **unique identifier** via a special `_uid` property. These identifiers are short, unique tokens (e.g., circled numbers like `①`, `②`) that are easily visible to the LLM but carry no semantic meaning beyond their function as a reference.

### 2.2. Targeted Operations

This `_uid` is then used to target all operations to a specific task's context. Furthermore, all meta-parameters for a `Tool Call` are prefixed with an underscore (`_`), and its `params` are inlined directly into the call object.

- **`Tool Call` Association**: Each `Tool Call` in the generated plan contains the `_uid` of the context it should operate on.
- **Implicit Scoping**: The `_uid` on a `Tool Call` implicitly scopes all path-based operations (`_outputPath` and input references) within that call. This means that when a tool reads from or writes to a state object, the path is relative to the `_uid` of the context it belongs to.

This mechanism allows the definitions of the tools themselves to remain simple and agnostic of the batching context. The `_outputPath` and input references within a tool's schema do not need to be updated; the identifier cleanly separates the operational contexts.

### 2.3. Example

A single request might contain two state objects for sentiment analysis:

```json
// Input to the Agent
{
  "context": [
    { "_uid": "①", "type": "state", "state": { "text": "This is wonderful!" } },
    { "_uid": "②", "type": "state", "state": { "text": "This is terrible." } }
  ]
}
```

The LLM processes both in a single context and generates a unified plan:

```json
// Output plan from the LLM
{
  "calls": [
    {
      "_tool": "analyzeSentiment",
      "_uid": "①",
      "text": "†state.text",
      "_outputPath": "sentiment"
    },
    {
      "_tool": "analyzeSentiment",
      "_uid": "②",
      "text": "†state.text",
      "_outputPath": "sentiment"
    }
  ]
}
```

The host environment then executes this plan, writing the results to the respective state objects.

## 3. Complementary System: The Planning Graph

While not a strict requirement for batching, the **Planning System** works symbiotically with this architecture to enable highly predictable, reusable workflows.

A **Plan** is a template for a process, defined as a directed acyclic graph (DAG) of `Tool Calls`. This graph is generated by analyzing the dependencies between tools reading from and writing to a `State Object`.

Crucially, this plan can be generated and perfected _before_ execution. Once finalized, the plan can be passed as a `Context Message` to the agent. When processing a batch, the agent can then follow this pre-defined plan for each `State Object`, achieving highly consistent and predictable results across multiple invocations. The `State Object` for each item in the batch serves as a snapshot of its current position within that execution graph.

## 4. Advantages of this Approach

This state-driven batching model provides significant benefits:

- **Efficiency**: It multiplies the throughput of the system by processing many tasks in a single LLM request, dramatically improving speed and reducing costs.
- **Consistency & Quality**: By allowing the LLM to see multiple related tasks in a single context, it can generate more consistent and higher-quality plans, leveraging patterns across the entire batch.
- **Predictability**: When combined with a pre-defined **Plan**, the system can achieve deterministic outcomes. The deterministic execution loop ensures that once a plan is followed, its outcome is reliable and repeatable across every item in a batch.
