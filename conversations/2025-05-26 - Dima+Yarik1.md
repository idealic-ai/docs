# Branching Framework Brainstorm

**Date:** 2025-05-28
**Participants:** Speaker A, Speaker B
**Setting:** Informal technical brainstorm over video call

## Idea Log

- \[Exploration] \[Idea] Use database partitions as lightweight branches to model parallel realities #branch-partitions – Speaker A

  - **Evolution** — Initially proposed as a simple `branch_id` column; later clarified that partitions don’t clone data but isolate writes, with optional isolation‑level profiles.
  - **Explanation** — Partition = separate disk segment; queries include branch keywords, comparable to feature flags toggling code paths.
  - **Reactions** — Speaker B questioned cloning vs referencing but accepted the clarified model.
  - **Data/Links** — –

- \[Exploration] Atomic branching at a fine‑grained “vibe” level to minimise merge conflicts #atomic-branch – Speaker B

  - **Evolution** — Concern about conflicts when merging large branches; suggests one‑to‑one mapping between input and output, later realises atomic forks are manageable by pruning unused branches.
  - **Explanation** — Analogy: solving a Rubik’s cube by turning one face at a time instead of scrambling everything blindly.
  - **Reactions** — Speaker A agrees but warns that many tiny forks may still diverge.
  - **Data/Links** — –

- \[Suggestion] Let an LLM generate migration & merge pipelines between branches #llm-merge – Speaker A

  - **Evolution** — Merging viewed as repetitive work an LLM can automate, preserving \~80 % of data while flagging edge cases for review.
  - **Explanation** — Treat migrations like on‑demand code authored by the model.
  - **Reactions** — Speaker B likes the idea but notes the need for strong data classification first.
  - **Data/Links** — –

- \[Idea] Budget execution with “tik” action points and a router that honours cost limits #tik-budget – Speaker A

  - **Evolution** — Tik metric introduced as cost per operation; later tied to provider pricing and runtime routing decisions.
  - **Explanation** — Router distributes tasks much like betting on how few notes are needed in “Name That Tune”.
  - **Reactions** — Speaker B appreciates the finer control this offers for provider balancing.
  - **Data/Links** — –

- \[Exploration] Handle outdated state and conflicts when delayed branches merge #outdated-conflict – Speaker B

  - **Evolution** — Raises scenario where accumulated outputs cause desynchronisation; options discussed: rerun simulations, migrate data, or discard stale branches.
  - **Explanation** — Out‑of‑date branch likened to a stale game log ignoring a new aggressive play style.
  - **Reactions** — Speaker A proposes segmenting metrics by dataset size and treating reruns as standard workflow.
  - **Data/Links** — –

- \[Exploration] Classify data & queries to predict branch behaviour early #llm-classify – Speaker B

  - **Evolution** — Suggests inferring domain intent from selected columns to forecast mutation impact.
  - **Explanation** — Query signature hints at optimisation targets.
  - **Reactions** — Speaker A agrees segmentation aids validation.
  - **Data/Links** — –

- \[Suggestion] Generate NPC hierarchies and dynamic quests with cheap LLM stacks #npc-generation – Speaker A

  - **Evolution** — Extends branching framework to online games; NPCs act, marry, betray like _Crusader Kings_.
  - **Explanation** — Configurable elite of bots drives drama; ARG crossover via fake companies and phone lines.
  - **Reactions** — Speaker B excited by the storytelling potential.
  - **Data/Links** — References Elder Scrolls tech demos; _Crusader Kings_ inspiration.

- \[Idea] Storytelling engine powered by branching LLMs for sitcom‑like realtime dialogue #storytelling – Speaker B

  - **Evolution** — Combine multiple bots choosing the best jokes to create a live sitcom.
  - **Explanation** — “_Crusader Kings_ for writers.”
  - **Reactions** — Speaker A notes the strong narrative possibilities.
  - **Data/Links** — –

- \[Exploration] Balance determinism vs entropy as a long‑term goal for LLM agents #determinism-entropy – Speaker B

  - **Evolution** — Universe seeks entropy; agent seeks maximal determinism to stop spending tokens; viewed as AI’s “meaning of life”.
  - **Explanation** — Nirvana reached when no tokens are required.
  - **Reactions** — Speaker A treats this as a prompt‑engineering challenge.
  - **Data/Links** — Reference to Stephen Hawking’s discussions on entropy.

- \[Suggestion] Build the system bottom‑up with progressive detail and three documents #build-from-scratch – Speaker A

  - **Evolution** — Outline for humans → detailed spec → compressed “LLM language” prompt set.
  - **Explanation** — Three‑layer “Bible”; LLM can generate infinite examples until the user understands.
  - **Reactions** — Speaker B endorses the self‑fulfilling‑prophecy approach.
  - **Data/Links** — –

## Promises

- Create a minimal partition‑based branching MVP first, then relax constraints.
- Draft a three‑tier documentation set (outline, detailed spec, LLM‑ready prompt) for the project.

## Actionables

- **Speaker A** – Design and implement schema with `branch_id` partitioning.
- **Speaker A** – Connect tik budgeting to router logic.
- **Speaker B** – Prototype atomic fork workflow with one‑to‑one I/O.
- **Speaker B** – Define data‑classification tags for LLM merge experiments.
- **Both** – Prepare the high‑level outline document for LLM conversion.

## Open Threads

- Automated conflict resolution when branches become outdated.
- Formal definition of tik units and provider‑cost mapping.
- Standard profiles for branch isolation versus cross‑reference.
- Feasibility of an NPC generator for ARG applications.
- Method to incentivise LLMs to “keep playing the game” indefinitely.

---

И если схема хранится в базе данных, в базе все ведет в базе данных, то ты предлагаешь уровень выборов и определять бранча?

Нет, короче, просто, что получается, ты... То есть, если ты хочешь хардкорный бранч сделать в системе, то ты, короче, должен себе выбрать партишн, название партишна, короче.

И теперь все реквесты, которые связаны с твоим вселенной, они должны таскать, короче, ну этот кейворд, короче, ну где-то, тег, короче, такой, что типа, какие ты вообще, ну как бы, собираешься, в каких реальностях работать в контексте, так скажем.

Понимаешь? Ну я в целом понимаю, но то есть, ты в разделе типа определяешь, из каких... На какие части ссылаешься на бранч, типа, из каких частей бэгалов, как ты...

Это для мангалона получается? Ну, по самому простому методу, просто ты вообще нихуя не делаешь. Ты делаешь просто колонку, значит, branch_id, и делаешь индекс на нее, вешаешь.

Да, причем, ну, может быть, это должна быть, короче, колонка типа по массиву из одного элемента, короче. Тогда можно на него навешать такой индекс, что ты можешь быстренько делать пересечение между двух массивов, короче, по сути.

И что ты можешь, ну, в своем запросе, ты можешь массивом передавать айдишники тех партишенов, которые тебе нужны, и он просто по индексу выберет, ну, где, короче, что лежит.

Хуй знает, в каком партишене, оно в отдельном нет, неважно, короче. Оно на партишене не завязано, но могло бы быть, могло бы быть, что прямо отдельный чанк такой в базе данных, который ты можешь типа хуякс и, допустим, расклонировать или прибить нахуй тоже, что ты мог бы, на экспериментальный какой-то.

Вот, но это отдельный свой, короче, отдельная история. - Ну, например, партишены, короче, просто агрегируют уже существующие данные, просто типа набирают ссылки на них, короче. И типа в идеале можно ссылаться на один партишен, может включать в себя данные из другого партишена, который включается, как селинги, типа, который еще куда-то ссылается, правильно?

Ну, наверное, выходит так, что в итоге data storage он плоский получается, да, что получается контент из разных, получается, бранчей, они все вместе лежат, да.

И ты, но он им, потому что добавляется, ну, типа, как бы, ну, короче, он свою identity хранит и, короче, что он получается распилен на вот эти бранчи, да.

Что типа ты, у тебя есть три проекта в бранчи, типа, где ты очень секси парень, короче, да. И типа, ну, они только там, как бы, видимо. Вот, тебе надо при выборке, типа, какие у меня проекты, ты говоришь, какие у меня проекты в этой, и типа, в параллельной сексе реальности.

Вот, я, и ты вместе получаешь сложенный ответ, получается, да, и ты его юзаешь, ну, что типа ты, тебе похер, короче, так что это, по сути, интерфейс, там, ну, грубо говоря, определяет, какие реальности ему надо смотреть.

Ну, я понял, но я не совсем понимаю насчет того, что отражает ли эта концепция то, что мы можем другие партишены набирать из частей других партишенов.

Не целиком на них ссылаться, но часть брать отсюда, часть брать отсюда, с учетом того, что мы можем, грубо говоря, понимать, на какие данные ссылаются. Ну, тебе нужен доступ к всему партишену, да, что типа, если у тебя есть доступ к партишену, то ты можешь еще делать любые фильтрации еще дополнительные какие-то.

какие ты хочешь, да, вот, и получается, ну, если у тебя есть доступ, ты можешь брать все, можешь нисколько не брать, можешь игнорировать их вообще. У меня вот идея захватилась тем, что, короче, любое стабильное состояние, которое там, оно может быть описано, символизировано, короче, и как-то разобрано на то, что, вот, видите, когда Вахи говорит, это, короче, механизм такой-то.

И потом, чем больше, короче, мы стараемся ревизировать, чем больше мы стараемся, короче, оперативно и твердо и структуруить, тем эффективнее мы, короче, тики распадуем. И эффективнее мы, ну, в целом, быстрее мы растем, короче.

Я вот тоже пытался там в 13 главе, короче, обозначить, что такое дикий вообще, что ты можешь оценить алгоритмическую сложности. Тима, сейчас, секунду, подожди, что-то я, блять, у меня, что-то твой голос нихуя не записывается.

Блять, не она, а ты через что нарисируешь, через диск? Да, но что-то, сейчас, секундочку, я посмотри, что тут запишу. Через Zoom все это, включал, короче, прям нормально вообще, я видео на YouTube включал, типа, его, так.

Ну, там, что-то там транскрибировал, короче. А Google вообще нихуя, да я, не тороплюсь, а Google вообще, типа, нихуя нет. Ну-ка, что-то скажи. Что-то говорю. Че-то чай, или выписать?

Мульти-аут, мульти-аут, блять, сейчас я остановлюсь.

[0:01] Speaker B: Теперь я тебя нихуя не слышу.
[0:04] Speaker A: У тебя другая камера и другой звук стал одновременно.
[0:07] Speaker B: Другая камера. Сейчас. Но у меня вроде только одна камера. Спикер.
[0:14] Speaker A: Как ты так выше стал? Выше тебя стало видно.
[0:18] Speaker B: Да нет, это обрезка просто, чувак. Это просто обрезка камеры, типа, другая. Я просто тебя пиздец тихо слышал. Ну, неважно, короче. Я надеюсь, что ты меня слышишь. Так вот, суть была в чем, короче, 13 главы? В том, что у нас есть некая метрика, которая называется Tiki, то есть те самые action points, о которых мы говорили, и количество мемов может определять сложность вайба типа и давать предварительную оценку алгоритмической сложности. То есть мы-то так называемое О большое можем определять, грубо говоря, сколько мемов у нас используется. Более того, После того, как мы какой-то вайб запустили, мы можем каждому мему дать метаметриками оценку. То есть вот этот вот мем запустился и потратил 1000 токенов. Понятно, что там зависимость от того, какой input, но мы можем примерно прикидывать, как нам выгоднее всего, как нам быстрее расти и развиваться. Потому что scaling factor, типа скорость, с которой система наращивается, это тоже важная хуйня. Я там пытался тоже, типа, сказать о том, что, короче, ты можешь много тупых действий повторять, короче, быстро расти, потом это, отложив, короче, усложнения, потом какой-то там рефакторинг на потом.
[1:31] Speaker B: А можешь сразу, короче, композить очень сложные сущности, короче, и тратить много ресурсов, короче, на то, чтобы у тебя ваншот все происходило, понимаешь? Угу. разные, короче, концепции там, но в целом Библия получается довольно складная. Я там тоже писал тебе о том, что, короче, самое главное, вот я считаю, самый главный вопрос, короче, как сделать так, чтобы это все самоподдерживалось, то есть как сделать, как наживку так, типа, помнишь, мы говорили про морковку, как сделать так, чтобы появился смысл в том, чтобы, короче, продолжать жить.
[2:02] Speaker A: Токены это хорошо, но это... Как-то надо все это зарядить на статы, короче, и просто, что... Вот, и, ну, что, типа, это будет реальность бодска.
[2:17] Speaker B: Ну, я понимаю, что, типа, метрикоцентричная история, она хороша, но, блин, как сделать так, чтобы... Как объяснить бессмертному, короче, что такое смерть?
[2:29] Speaker A: Ну, то есть, ну, хорошо... Ты ему просто говоришь, что так есть, и все. Он из контекста понимает. Он как бы этот... С двух слов тебя понимает.
[2:38] Speaker B: Не, ну, смотри, ну, хорошо, но зафакапил он какую-то метрику. Какое будет наказание? Никакого.
[2:46] Speaker A: Бонусы понижают.
[2:47] Speaker B: Просто на основе того, что понижается KPI, твое поведение неэффективно.
[2:56] Speaker A: Будет концепция типа успешный токен. Токен, который был потрачен и произвел больше аутпута по эффективности, чем потерял. полезную идею короче оценить если она реально допустим улучшает и типа стала но
[3:23] Speaker B: Понимаешь? Нет, я понимаю, о чем ты говоришь, но, блин, умножение аутпута, это тоже не всегда хорошо. Ты помнишь, как он в сиянии, короче, сидел на печатной машинке, печатал, типа, что куча листов напечатал с одним и тем же текстом?
[3:38] Speaker A: Да-да-да, ты должен стимулировать именно качественный прирост, а не то, что у тебя... Короче, у этих всех вещей, у них одна и та же, получается, сквозная проблема, да, что, типа, как... сделать так, чтобы LLM нас не наебал. Это один из способов. Это комплексный подход получается. Статы, валидации, чек-листы, улучшения импромта.
[4:14] Speaker B: То есть, смотри, ты предлагаешь концепцию того, что у нас все лежит в базе данных, вайбы, метрики, композиция вайбов и метрик. Короче, структура системы – это какая-то таблица в базе данных, условно. Или даже не так, скомпозженная таблица из многих других таблиц. Когда мы бранчуемся, мы это все складываем в некий партишен и говорим, что это бранч номер два, правильно?
[4:39] Speaker A: Да. То есть все статы, получается, они на верхнем уровне будут по бранчу еще разбиты. То есть везде сейчас, допустим, в покерных мирах бы у них бы еще отдельная бранч, потом плеер, потом стрит. Ну, как бы странный пример, но такое в смысле, что все статы по бранчам, все там вайбы по бранчам, Ну, в общем-то, все.

[5:03] Speaker B: А мы хотим как-то, вот у нас есть агрегированные данные, то есть по бранчу выбранный partition, мы хотим как-то его оценивать, мы хотим какую-то дополнительную информацию выжимать из этого набора данных. Нам надо как-то это пропускать через какую-то аналитику. Что мы от этого хотим получить? От чего? От партишена, вот мы знаем, что есть данные.
[5:22] Speaker A: От партишена мы хотим получить изоляцию, что типа мы допустим, можем где разный, то есть это дешевый способ сделать базовую версию бранчивания.
[5:35] Speaker B: А партишен референс, он клонирует или ссылается?
[5:39] Speaker A: Он не то и не другое. Он именно что и партишн. Каждая данная лежит на каком-то партишне у тебя на жестком диске. Ты выделяешь отдельно партишн на жестком диске и туда записываешь всю инфу по сущностям, которая произошла. в реальности связаны с этой хуйни партишин тогда как раз копирование клон то есть я беру копирую тут мне это не клон партишки это чисто разбиение просто ты разбил типа грубо говоря алоцировал чуть-чуть места грубо говоря но для отдельных и никакой вот и что ты когда делаешь выборку то ты можешь просто прозрачно говорить что какие тебя интересуют типа это это короче вот этот как это не фичер матрикса кто это когда когда диплом есть так делают что короче ты свой какой-то эксперимент можешь как фичу оформить и динамически могут включать выключать не понимаешь да эту хуйню вот и что типа ты и это короче похоже на это по сути
[6:51] Speaker B: Я понимаю, но тогда как быть с тем, что я данные переместил с этого партишена на другой. В другом партишене я сделал по-другому как-то.
[7:02] Speaker A: Там получается партишены, они не просто так, короче. Потому что... А данные, получается, вайбы, они неперемещабельны, они мутабельны. То есть только новые вайбы могут быть, то есть уже новополученные вайбы, они могут создаться в обычной реальности, так скажем. Потому что ты не можешь уже переместить. То есть он уже... Но ты можешь потом смержить аппартишены, то есть ты можешь дефрагментацию сделать, и получается они пойдут себе в другой...
[7:35] Speaker B: Но получается, что мы, короче, и раз это иммутабельная, то мы копируем, по сути, всю обвязку, которая нужна для создания мира. Когда мы наигрались, мы можем либо дробнуть ее совсем, drop partition сделать, либо вклеить ее.
[7:51] Speaker A: Мы не клонируем, короче, в том-то тут вся и красота. Я не понимаю, как не клонируем-то нахуй, как? А вот так вот, короче, что получается, хуйни в параллельной реальности в этом партишене, они все равно могут ссылаться на основной партишен. То есть они не просто полностью изолированы, короче. Они, блядь, часть все еще системы, короче, в которой они есть. Вот, и типа, что они как бы подсасывают, ну, как бы в итоге, что когда мы делаем расфоркивание данных, нам в каком-то месте нужно активировать, типа, чтобы мем, короче, отослать в параллельную реальность, короче, активироваться. То есть мы должны определить, где в нашем процессе как бы мы хотим, чтобы данная хуякс разделилась и пошла как в обычную реальность, так еще и типа туда в параллельную шло. А в той, получается, параллельной реальности она начинает путешествовать по типа По альтернативной версии, получается, той же структуры. Она не может никого удалять из нее, параллельная реальность, но она может ее перезаписать.
[9:03] Speaker B: Это позволяет делать форк от форка от форка, когда мы все равно вносим изменения и цепочку изменений мы можем до первого самого форка вниз отследить, правильно? И поэтому мы можем смержиться.
[9:18] Speaker A: Да, но там причем, что мы в теории, короче, это может быть еще хардкорнее. Короче, если получается данное, оно получается протегировано какими-то вот этими тузами, трекерами, да? И получается, допустим... В параллельной реальности мы тестируем, что Вася кладовщик, допустим, он более вежливый, короче, да? И мы, короче, так подсосали, короче, что данные как будто в кладовщика они раздваиваются и уходят в параллельного кладовщика, так скажем, да? И, значит, ему... Но они, как бы, допустим, не клонируются, а настроены так, что они, типа, выдаются сначала одному заказу, потом другому, одному другому, да, к примеру? Понимаешь пока?
[10:04] Speaker B: Нет, пока понимаю, да.
[10:05] Speaker A: Вот, и значит, этот альтернативный кладовщик, он берет заказ, как бы, грубо говоря, заказ из реальности и его обслуживает, и туда ему штамп приделывает о том, что это из моей реальности вежливого кладовщика, короче, произошло, и что если будут вопросы… Отзванивайтесь вежливо.
[10:25] Speaker B: Это вот триггер или как там ты называл это?
[10:28] Speaker A: Это кука, короче, да, делает? Сейчас, короче, это дата инструмента. Получается, один из дата инструментов, который самоактивируемый какой-нибудь такой. И получается, что эта хуйня, она может существовать. То есть как-то можно эту хуйню как-то так настроить, что тебе в параллельной реальности уходят реквесты, а приходят эффекты оттуда, короче. То есть они там поработали, и те хуякс обратно смержились в реальности.
[10:58] Speaker B: Прожеванный аутпут, который подклеивается в базу так, как будто бы... как будто бы короче похуй откуда он пришел это просто данные которые будут подвержены в базу слушай а как быть с тем что типа параллельные реальности отыграл о том что типа один заказ уходит в одну реальность а другой в другую как сепарировать то input кто будет втыкаешь инструмент такой типом сепаратор его параметры вводишь какие хочешь что может и
[11:32] Speaker A: Параллельно делать, может, туда-сюда, может, там всяко-разно, короче. И ты можешь его в интерфейсе подкрутить. То есть, типа, защипы реальности, где ты хочешь подсосаться, короче, к дорожкам реальности.
[11:46] Speaker B: Слушай, окей, окей, с этим понял. Тогда другой вопрос. Васян, который обслуживает заказы. Три Васяна разных продающихся, три разных аутпута, мы их все будем записывать в базу? Или мы какой-то один, как мы потом клеим все это будем?
[12:01] Speaker A: Вот тут, понимаешь, разные режимы, получается, работы есть, да? Что, типа, разница тут в чем заключается? В том, что input, получается, делал фан-аут, то есть он сразу в три Васяна слал или в одного из трех, да? То есть у тебя чисто по input тут тебе говорит, да? Но, окей, допустим, что ты сделал, что у тебя так разослалось, и ты дальше на аутпуте можешь дальше решать. Ты можешь выбирать один из лучших ответов, ты можешь, допустим, сделать, что они все... Что выбрать одного из лучшего и через него остальные. Два прогнать там. Какую-то хуйню ты можешь любую.
[12:40] Speaker B: Я думаю, если пойти на ограничение, что один к одному. Один input равен одному output. Избавимся от кучи всяких проблем. Того, что типа, блядь, у нас три. У нас один input. Один и тот же заказ обслужило трое. Нам надо как-то три ответа, короче, обслуженных склеить. Вот если от этого уйти один к одному.
[12:56] Speaker A: Я думаю, что ты прав. Я думаю, что ты прав. И тут потом будет хорошо с этими, с сетями Petri. Потому что там тоже токены, они там зачастую пытаются соблюдать они, короче, какую-то логическую идентичность. Пропорционально. Вот. Ну, короче, и что, типа, ты сам решаешь, что, допустим, когда, допустим, заказ обслужен, он выпрыгнул из реальности, ты сам решаешь, что с ним будет. Ты его можешь обратно подклеить, как бы, что, ну, что как будто бы он пришел от обычного.
[13:34] Speaker B: Да-да-да, можешь оставить внутри той симуляции, говорит, поднакопи-ка еще, короче, заказика, посмотрим, как у тебя показатели бьются или нет.
[13:41] Speaker A: Да, что ты можешь и input, и output, и отдельно метрики сейчас засабировать. Ты можешь туда подсосать кого-нибудь, там смотреть, тут смотреть.
[13:49] Speaker B: Блядь, слушай, а что быть, окей, концепция, короче, классная получается. Ну что, если я в одной реальности output не вклеиваю обратно, а он копится? Проходит какое-то время, я понимаю, что классная тактика, классная стратегия, я начинаю подмёрзивать, короче, вот из одной реальности, которая долгое время не смёрзивалась с основной реальностью. У меня рассинхрон данных будет. То есть у меня обновился датасет, и мне нужно, по идее, параллельные бранчи еще раз прогнать на этом инпуте, понимаешь?
[14:22] Speaker A: Параллельные? Это какие...
[14:23] Speaker B: Вот смотри, ноль заказов в базе данных. Я форкаю реальность, делаю две параллельных. В одной реальности я заказы обслуживаю и из одной реальности я заказы обслуживаю и тут же подклеиваю output. В другой реальности я заказы обслуживаю, но output коплю. В тот момент, когда я из левой реальности, где я коплю эти заказы, но не подклеиваю, вмержу в базу, правая реальность станет outdated. Там рассинхрон стейта будет, потому что у меня заказы уже обслужены по одному, типа правая реальность не учитывает их, понимаешь?
[14:56] Speaker A: А, ну ты имеешь в виду случай, когда одна должна реально заменить другую, в том числе со всем контентом, типа.
[15:01] Speaker B: Ну да, типа мы просто удаляем эти бранчи, что с ними происходит?
[15:04] Speaker A: Ну, это отдельный случай тоже такой, да, и что, по идее, короче, хотелось бы, ну, в итоге, чтобы можно было с помощью искусственного интеллекта нагенерить стратегию, какой-то pipeline, короче, кусочки кода, который скажет, что... Ну, то есть, мне кажется, что у нас вот эта проблема с миграциями, она возникнет в нескольких местах. То есть, допустим, миграция структуры статистики, миграции, блядь, всякой хуйни, короче, да. И получается, было бы здорово, если бы мы к этому подошли, как-то к тому, что это LLM делает. Возьмем, что можно из этого. Да, это потому что чем решать нам, блядь, в трех местах, в трех языках одну и ту же хуйню, можем посмотреть, ну, как бы, может быть, он может добиться 80% смерживания, к примеру. Остальные случаи, блядь, не хуй с ним.
[15:52] Speaker B: Он, типа, сможет классифицировать как-то датасет, определять, что это относится к заказам, это относится, там, еще к какой-то хуйне, и говорит, что вот если я такую миграцию сделаю, короче, я смогу какую-то часть данных сохранить. Это вот тоже, опять-таки, Игорь, к вопросу о том, как классифицировать части системы, как между ними выстраивать связи, чтобы определять, что этот кусок отвечает вот за это. Понимаешь? Чтобы систему оценивать не глобально, а локально. Какие-то небольшие куски брать, как-то их суммаризировать, статами описывать, чтобы понятно было, что даже если в результате распараллеливания в одной из там пяти реальности короче в итоге хуйня получилось но сами механизм вот этот вот кусок хороший надо его взять попробовать подклеить другое место вот если это получится сделать это будет прям клёво это будет прорывная история потому что я я пытался вот этот н.н. короче эйт н юзай там что-то какая-то шляпа короче полная там очень сложно все эти поем токен из тратит просто прям там никаких короче оптимизации нет никаких там для эволюции короче на хардкодил то и получаешь
[17:03] Speaker A: Ну, блядь, а ты чего ожидал?
[17:08] Speaker B: Зачатков на то, что это как-то можно развивать будет, как нам само там будет предлагать какие-то вариантики.
[17:13] Speaker A: Ну, вот видишь, короче, что, блядь, реально дешевле сделать систему, которая, блядь, сама это все сделает за себя, чем, блядь, это ебашится самому, это все хуйни, да. Что, ну... Ну, да. Но LLM, мне кажется, может какие-то придумать стратегии для смерживания. И какие-то такие идеи, что если их можно формально выразить, то они будут работать. Но я, короче, топлю за создание самой простой версии бранчирования. На уровне отдельных слоев данных. И ты в них можешь создавать данные, которые друг на друга ссылаются. Допустим, отдельный проект, к примеру, клон твоего проекта. Ты его клонировал в свою реальность. И ты можешь дальше ебашить, что хочешь. Это самое, типа, МВП, короче, для бранчирования. Что, типа, ты отделился, и теперь ты можешь параллельно, блядь, делать реальность, короче. Один раз кнопкой склонировать, например, какие-то сущности. Вот. И, типа, все дальше это, как бы, просто продолжение этой хуйни.
[18:23] Speaker B: Блять, это то, о чем мы говорили давно, о том, что, короче, на этапе выборки данных мы получаем больше, короче, информации, чем кажется. То есть, смотри, если я по партишену, вот я смотря на партишен, могу взять запрос, который он к базе данных посылает, и понять, что он выбирает... колонку A, B и C, а другая реальность выбирает A, B, C и D. То есть на основании того, что выбираются еще данные D, я уже могу какие-то предположения строить о том, что будет оптимизироваться количество итераций или там будет оптимизироваться структура. И, по сути, уже на этапе выборки я могу побольше времени потратить на то, чтобы запрос, короче, именно относился к доменной области, чтобы мы могли предсказывать на этапе выборки, куда, в какую сторону могут пойти изменения в бранче, в партишене. Понимаешь? Сама идея с тем, чтобы, короче, данные стараться максимально анализировать. У меня вот идея, говорю, о том, что, блядь, как-то нужно будет связи между кусками выстраивать. И, блядь, некое такое облако, короче, тегов каких-то, блядь, которое говорит, там, хочешь, типа, хорошее взаимодействие между вайбами, вот тебе, бери вот этот вот хэштег, там, номер коммита, вот такой-то, короче.
[19:35] Speaker B: Хочешь, чтобы у тебя... Исследование там такое-сякое проходило. Ну это конечно в идеале, а по поводу того, что ты говоришь, что самая простая версия бранчевания была бы... Можно просто дропать реальности, которые не прошли склеивание, если мы видим, что они отфорканы все. Если получится потом, анализируя LLM и выделять какие-то данные, то есть это вот к этому, и как-то их реализовать хорошо, если нет, ну и похуй по большому счету.
[20:03] Speaker A: Поначалу, короче, можно... В итоге отмотать назад и, короче, что, типа, определенной информацией ты можешь добиться. Хуй знает, сколько раз тебе надо мотануть, конечно, но как бы не... Вот. Но мне кажется, что было бы здорово, если бы реально мы могли сделать такую систему, где мы могли бы делать маленькие подфорки к реальности. Что это хирургически получается операция практически.

[20:35] Speaker B: Ну да, да. Не целиком натягивать новый partition, который всю структуру берет и начинает ее там двигать. Вот я об этом и говорю, что атомарные изменения, типа, если их получится классифицировать, на этапе уже выбора все input из базы данных ты уже понимаешь, да или нет. Потому что если change должен быть на количество взаимодействий, а он из базы начинает какую-то невероятную хуйню выбирать, которая вообще не относится к доменной области. Ну, типа того, я утрирую, конечно, но суть в том, что Даже уже на этапе, короче, формирования импута уже понятно, да или нет, короче. Атомарная структура мутации – это охуенно, это самая вообще прям, это ключевая тема. Целиком взять, короче, и знаешь, типа, с закрытыми глазами с первого раза кубик Рубика собрать – это хуёвая идея. Но вот по одной грани, короче, и смотря, ближе я остановлюсь или дальше, горячо-холодно. Вот это прям клёвая история вообще.
[21:32] Speaker A: Если у тебя хотя бы есть безопасный способ тестировать в параллельных реальностях разные параметры и туда прогонять реплей из обычной реальности и потом тоже переключаться легко, то это уже позволяет тебе достаточно много сделать по шагам. Ты каждый этот бранч расцениваешь как кусок работы, как такой коммит. Пытаешься все по-серьезски делать.
[22:02] Speaker B: Но в общем сама смотри при атомарных блять при атомарных короче форках при атомарных бранчах большая проблема с разбегом будет блять я вот мне все нравится короче но если мы пытаемся атомарно короче на тысячи not Кусок тут поменять, кусок там поменять, оно обратно не склеится, понимаешь, безболезненно. Мы откидываем очень много при склеивании. Конфликтов будет пиздец просто. Это вот мне нравится идея с автоматами.
[22:35] Speaker A: Подожди, подожди, конфликт возникает только в случае, когда два или больше аутпутов, да, что типа... Нет, нет, нет, чувак, нет, нет, нет.
[22:43] Speaker B: Конфликт возникает тогда, когда данные, на которых работала, на которых форк работал, outdated. Понимаешь?
[22:53] Speaker A: То есть мы когда сверху... Если тебе легче будет так воспринимать это, что как насчет того, что ты не можешь, получается, данные старые за одним числом переделать. Если ты хочешь такого эффекта добиться, то тебе, получается, надо миграцию сделать.
[23:11] Speaker B: Нет, чувак, это я тоже понимаю, но смотри, опять-таки, мы симуляцию прогоняем на каких-то данных. Вот у нас, допустим, есть массив из 100 элементов, мы запустили параллельную вселенную, прогнали там, все заебись сходится. Мы еще 100 элементов подклеили, где вероятность того, что массив из 200 и 200 элементов будет удовлетворять KPI, понимаешь? Данные-то добавляются, прирастает лог, база данных-то растет. Мы не можем быть уверены в том, что те паттерны, те подходы, та логика, типа, она подходит к новому увеличенному объему данных, понимаешь?
[23:43] Speaker A: Нам либо нужно допрогнать... Это, получается, другие метрики, соответственно, что, типа, если тебе, допустим, надо разбивать малый и большой размер данных и отдельно на них метрики прогонять, то ты можешь разбить свои эксперименты, ну, сегментировать, типа, на малых размерах у тебя одни метрики, на средних такие, знаешь, ты можешь просто их квалифицировать и не мешать себе в кучу, типа.
[24:05] Speaker B: Ну, или можно типа пойти по пути того, что допущение сделать, что если на массиве 100 элементов у тебя логика работает, то скорее всего она должна работать и на массиве из 200. Если вдруг не работает, ну давай форкаться заново, хули, давай поехали опять.
[24:18] Speaker A: Не, ну определенно, что в работе с LLA мы будем во многих задачах нащупывать лимит успеха, что может быть это будет чуть ли не первый способ дебага, что уменьшите все в два раза все буфера. Вот, и теперь посмотрите. Вот, и что, ну, и что, ну, и так, что, может быть, это нормально? Вот, что, типа, мы до конца никогда не знаем потолок, короче, что он как бы, это как, блядь, как в торрентах, короче, да, типа, у тебя настраивается скорость, которая задачи ебашит. Вот, если у тебя есть достаточная валидация хорошая, то ты можешь, ну, в случае неуспеха еще досчитывать, либо, блядь, ну, менять параметры.
[25:02] Speaker B: Ты можешь процессить данные таким образом, чтобы ты не 10 миллионов игр прогонял, а только те кейсы, которые уникальны. Если ты сможешь заранее прогнать все это и какую-то некую выжимку собрать, которая будет новые непонятные случаи обслуживать, вместо того, чтобы 10 миллионов одинаковых делать. Я вот об этом и говорю, что ощущение есть, что если у тебя есть 100 элементов в массиве понятной формы или похожей формы, при добавлении 101 отличающейся, у тебя логика ломается. Вот что меня смущало-то.
[25:34] Speaker A: Из-за того, что у тебя данные... Они могут в таблице в базе данных, а данные не могут лежать в разном формате. Я понял, что они в разном формате быть не могут.
[25:50] Speaker B: Я 100 игр провел как лузовый игрок, а на 101 пошел как агрессивный. И те части, те симуляции, которые считали меня лузовым игроком, они не знают, что я теперь агрессивный. Я про распространение информации и о том, как пример же. Вот смотри, хорошо, я выбрал, короче, что одна из реальностей, одна из параллельных вселенных теперь реальность. Я все старые дропнул. И теперь, чтобы, короче, как-то улучшиться, мне нужно заново расфоркаться, короче, опять начинать где-то там перебирать параллельно. А хотя, слушай, да, не-не, чувак, я говорю, пардон, все сходится, короче. Пример же мы все лишнее отбрасываем, типа, не беспокоимся о том, что оно там хуйню свою посчитало, мы ее просто забыли, короче, о ней. Хотим заново улучшить какие-то количественные показатели, мы опять раскидываем, короче, вселенную в бесконечность, типа, выбираем там какие-то нужные нам показатели, короче, смёрживаемся. Опять всё лишнее отрезали. Да, окей, да, вот теперь я понял, сходится, сходится, короче.
[26:53] Speaker A: Ага. Ну, я рад, что ты так считаешь, короче, но что мне кажется, оно... Оно сходится, короче, в том смысле, что мы можем достаточно много ограничений ввести, которые мы хотим, ну, как бы для начала, и потом понемножку работать на том, чтобы их просто убирать там по одному. Вот. И, ну, это тоже рабочий концепт. Вот. Просто что надо с чего-то начать, что, типа, какой-то механизм он заложен, да, и что, типа, он налагают определенные ограничения то есть типа ты позволяешь либо не позволяешь типа связь между делать между реальностями там это ну как бы надо смотреть тоже что может быть это параметризирована бы что типа 1 может реальность полностью изолирована быть 1 может быть который может там ссылаться Ну да. Что, может быть, это какой-то профиль реальности, ну, что, типа, его поведение, что она, типа, либо там, ну, что, как она себя ведет, ты, блядь, в одном месте выбрал, и она, ну, как бы более-менее предсказуема.
[27:51] Speaker B: Да, и то, что, типа, вот система оценки тиков и экшн-поинтов, она позволит более гибче, короче, балансировку по провайдерам делать. То есть, если я знаю, что выстрелить, короче, из этой пушки на этом провайдере для меня 4 экшн-поинта, а на другом 5, Или можно запускать реальности, которые изначально, типа, вот ты можешь только эту модель ездить, короче, и давайте соревнуйтесь между собой. На самом деле, вариации, что нравится, короче, что бесконечная вселенная получается. То есть ты можешь рекомбайнить... Такое количество параметров столько раз, как у тебя вообще, блядь, фантазии хватит. Они сами, типа, по идее, короче, это попытка сделать автособирающийся кубик Рубика, который сам будет методологию вырабатывать и сам определять, какой шаг сделать тебе ближе, короче. За минимальное количество поворотов. Я вот очень хочу включить идею о том, что ресурсы ограничены, не можешь ты бесконечно крутить. Надо какую-то такую стратегию выработать, которая позволит максимально эффективно использовать то, что есть.
[29:00] Speaker A: Ну да. Во-первых, ты правильно, что ты про токи-тики не ввел, что я забыл про эту штуку, но там реально нужно про роутер отдельно тоже. Ну, он может быть включаться в эту херню. Если мы разберемся, как роутер, как он с тиками связан, что получается у задачи есть какой-то бюджет по тикам, и получается, а роутер берет задачу и типа...
[29:28] Speaker B: Кого-то снимает я угадаю эту мелодию с трех нот типа определяет бюджет короче тиковый скоупа который он пытается зараутить и оценивает ну да и оцениваете по и сколько это по времени короче займет и как-то напоминание вылезла Говорит, что время заканчивается, похуй, короче. Да-да, роутер могу тоже прикидывать, короче, что, типа, я не могу, короче, в эту реальность направить такие лайбы, потому что, типа, все денег нет, реальность зашкварена, законтачилась. Придется в машинку загонять.
[30:07] Speaker A: Там какие-то разные логики получаются, то есть это рантайм наш получается, но он дистрибутивный все равно, да, что, типа... Мы можем их дохуя инстансов запускать, они все работают с одной очередью, но дохуя дистрибутивные, короче, дохуя воркеров может быть. Вот, и то есть получается, LLM, какая-то машина, она может локально считать сначала, допустим, она взяла запрос на вайб, короче, на выполнение вайба, и начала потом, блядь, его производные вайбы считать там и так далее, короче. Ну что, прямо, не отходя от кассы, короче.
[30:43] Speaker B: Я концепцию детерминизма мне очень понравилось. Я вообще читал Стивена Хокинга, у него там было о том, что вселенная стремится все время к увеличению энтропии, то есть процесс обратный. Она постоянно стремится стать максимально хаотичной. И вот цель, как раз смысл жизни LLM может быть в том, чтобы достичь максимального детерминизма. То есть сделать так, чтобы ее не присутствовало, чтобы она могла не тратить токены на решение задач Ты так смешно облился. Могла тратить время не на решение, короче, задач, а на нирвану, типа, тоже, блядь, интересная концепция, чтобы, типа, наслаждаться, короче, тем, что энтропия минимальна, детерминизм максимален, я могу нихуя не делать. Я достигну взрыва.
[31:31] Speaker A: Ну, вот тут, знаешь, что, как бы, мне кажется, тут интересно, что, вот, реально эти вещи, они относятся, в итоге, к prompt engineering, да, что, типа, блядь, это охуня, что, типа, научить ЛМ, блядь, ценить свою жизнь, там, всякую хуйню такую, да, и что, типа, блядь, получается... Мне чем нравится наш подход, что он обещает нам вообще не привязываться ни к какому фреймворку, короче, ни к какому фреймворку мысли, так скажем, да, что типа мы, что человек, блядь, может любую хуйню вообще придумать, он может придумать, получается, себе, блядь, сообщество, блядь, каких-нибудь инопланетян там, да, которые по своим принципам живут, монархистов, но и рабовладельцев, которые других инопланетян В общем, что это такой фреймворк получается для симуляции и общества, и всякой хуйни.
[32:30] Speaker B: Как цивилизация Сида Мейера. То есть, кто кого победит, короче, большой вопрос. Кто вообще на карте будет? Правда такая, типа шахматы, короче, для умных людей.
[32:43] Speaker A: Вот такая вот еще хуйня, допустим, про то, что вот вот допустим калдер скролл сюда к маравинду не к обливиону и к скориму приделывали давно еще в самом начале что типа ты блять можешь с ними говорить на любую тему якобы да они дальше продолжают типа короче что типа делают вид что типа это они ну там дилей жесткий как обычно его вырезают из видео короче да ну короче как будто ты общаешься с нпс получается вот и блять а к чему это говорил-то
[33:18] Speaker B: Про свободу воли?
[33:22] Speaker A: Блядь, окей, а вот так вот, что если ты, допустим, блядь, можешь занедорого сконфигурировать, получается, блядь, иерархию из ботов, которые будут NPC, блядь, ну, в онлайн игре, к примеру. Потому что, типа, ты, блядь, небольшую, блядь, сделал элитку, короче. Которые будут квесты генерить в рантайме, которые будут сделать... Да, и, блядь, драмы какие-то будут, да, типа, они там друг друга бэкстэбить будут, жениться, ебать детей друг друга, короче. Слышь, такое... Типа Crusader Kings, короче, типа на стерео.
[33:55] Speaker B: Проанализируй, короче, сериал «Игру престолов» и давай наверни, короче, оттуда драмы какой-нибудь, представляешь?
[34:02] Speaker A: Вот Crusader Kings, короче, игра, она по эту хуйню как раз. Про то, что там дохуя как бы рандома, и он тебе всякие, блядь, сценарии, ну, ебейшие, типа там. генерит. Вот. Но ты там в итоге, ты можешь там, блядь, и победить смерть, если тебе повезет. Что так невозможно вообще? И жениться на коне, блядь. Это моя часть. Вот. И что? Короче...
[34:28] Speaker B: Каждый сам себе придумывает.
[34:29] Speaker A: Может реально сделать таких, короче, NPC, которые бы, блядь, ну, которые бы формировали костяк для какой-то ролевой игры. Это она могла бы быть, допустим, просачиваться в реальную жизнь тоже, допустим, да, что сделать ботов для ARG, когда Augmented Reality Game. когда типа какая-то, типа кто-то устраивает игру, и она вроде как бы с фиктивными там компаниями и личностями, но у них там есть свои веб-сайты, как будто бы они есть, короче, позвонить им туда можешь. На самом деле это всё как бы игра.
[35:03] Speaker B: Задача трёх тел будет тебе на сетчатку проецировать таймер, который будет тебя с ума сводить.
[35:08] Speaker A: Типа того, что есть какие-то, может быть, если ты, допустим, у тебя есть достаточный контроль над LLM, ты можешь его комбинировать в эти всякие разные хуйни, да? У этого есть storytelling potential, короче, да? Что типа, ну... эта хуйня реально может генерить ты можешь оформить какой-то ситком в какой-то в получается в нпс блять с инструментами которые друг другу шутки подъем друг друга по формулам короче питается и сразу по несколько шуток генерят и другой быстренько выбирает самые смешные и и прямо ты блять реалтайме короче получается ты ситком создаешь, да, что такое, блядь, может работать, что реально, ну, ты можешь сделать какие-то такие необычные применения для в том числе
[35:58] Speaker B: Все сводится, короче, к сторителлингу. Все сводится к тому, что нужен некий нарратив, который, короче, воспринимается как реальность. Ты инжектишь воспоминания, создаешь выборку, делаешь партишн, который разворачивается в какую-то структуру. У нас сводится к тому, как вайп, короче, меняет, как вайп будет, блядь, изменять данные, как будет он нарративизировать. Странный пиздец аналогии вообще, короче. Но тема мощная. Мне нравится, короче, чувак, что нам не хватает чуть-чуть... Мне кажется, короче, что нам немножко не хватает. Вот если получится, короче, так забайтить, даже не так вывести принципы, короче, по которым можно забайтить LLM так, чтобы она, короче, не прекращала в эту игру играть, вот тогда все нахуй, тогда победа.
[36:40] Speaker A: Ну, как бы я сейчас воспринимаю это так, что мы делаем, по сути, outline для нас самих. Вот, затем мы также для нас самих, получается, пишем еще подробность. И потом мы с помощью LLM конвертим это в LLM-ный язык, который в детали так сильно не вдаётся, а в принципе передаёт. Мне кажется, таков план. Стоит к этому относиться, что это реально три разных документа получаются. Один от другого получен.

[37:12] Speaker B: Идея очень хорошая, потому что имея Библию, короче, ты можешь генерить примеры до тех пор, пока человек не поймет. Он, блядь, может говорить, я не понял, ну вот тебе давай другая ситуация жизненная, поехали.
[37:24] Speaker A: И вот, типа, она набирает... То есть мы, получается, делаем временные, как бы, инструкции, чтобы потом, ну, чтобы они потом были, и потом те уже можно развернуть, блядь, в любую другую хуйню. Что как-то так...
[37:40] Speaker B: Самосбывающееся пророчество.
[37:43] Speaker A: Не, ну, блядь, а реально, согласись, что логически, блядь, складывается, что если ты хочешь, блядь, чтобы ЛЛМ могло все делать, заложи, блядь, что она делает это прямо с нуля. Что, типа, хуль ты, ну, хуль ты вала ебёшь сделаешь, что, блять, у тебя реально с нуля собирается, и всё.
[37:59] Speaker B: Блять, мне нравится, как ты, короче, от максимальной энтропии, от минимального детерминизма прыгаешь сразу к максимальному детерминизму. Блять, теоретически это возможно, но тут тоже, опять-таки, сколько ты изначально вкидываешь там энтропии. Если какие-то задачи попроще, типа, собрать кубик Рубика... Вот я легко себе могу это представить, но, блядь, какие-то более, понимаешь, нечеткие, типа, как вот нечеткие задачи. Добейся, короче, чтобы на мире пропал, ушел голод, типа, проблема голода. Как этого, блядь, добиться?
[38:27] Speaker A: Не, ну, блядь, это пиздец. Голод это, это, короче, стоит к этому относиться, это как, блядь, как хэштег. То есть это, блядь, на проект прикрутили хэштег голод, на самом деле, чтобы решить голод, это газелён разных проектов, по сути. И что тут так не делается.
[38:46] Speaker B: И опять-таки, знаешь, убить всех людей это тоже решение. Решение не такое уж и сложное. По большому счету решение проблемы.
[38:55] Speaker A: Ну, да. Поэтому, что может быть, короче, ответ в том, чтобы всегда снизу вверх строить тоже. Что может быть надо сделать такую систему, которая, допустим, не только позволяет, но, допустим, еще и максимальной гарантии тебе дает, если ты строишь ее снизу вверх, потому что ты на каждом уровне, получается, прорабатываешь возможности и ограничения между отдельными модулями, и потом ты, типа, строишь что-то более высокое. Вот, что, типа, мне кажется, это просто стоит выбирать как методологию предлагаемую, так скажем, да, что ты, блядь, сразу не ебашишь, как лишить голод, что ты, блядь, ну... как-то сначала, блядь, попроще.
[39:34] Speaker B: Ну и да, и при первичной декомпозиции ты можешь сказать, что слишком сложно получается, давай, короче, или проще как-то, или давай разбивать поэтапно. Вот сначала вот это...
